{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install bs4\n",
    "!{sys.executable} -m pip install sklearn\n",
    "#to install Tensorflow: https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Load and Save Functions\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#Functions for loading and cleaning the Wikipedia Data\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "#SKlearn packages for pre-processing the train and test data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Keras Packages for creating the deep learning model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "    \n",
    "print(\"Import Successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model Saving Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Git does not allow empty folders to be checked in and out so we will create the model saving directory\n",
    "try:\n",
    "    os.mkdir('Baseline_Models')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir('EntityPlus_Models')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train and Test Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data(file):\n",
    "    with open(file) as f:\n",
    "        reader = csv.reader(f)\n",
    "        S = [[row[0],row[1]] for row in reader]\n",
    "        S.pop(0)\n",
    "    return S\n",
    "\n",
    "def Load_Baseline(f1,f2):\n",
    "    train = Load_Data(f1)\n",
    "    test = Load_Data(f2)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Wikipedia Data through Web API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Wiki(wik):\n",
    "    raw_html = urllib.request.urlopen(wik)\n",
    "    raw_html = raw_html.read()\n",
    "\n",
    "    article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "    article_paragraphs = article_html.find_all('p')\n",
    "    article_text = ''\n",
    "\n",
    "    for para in article_paragraphs:\n",
    "        article_text += para.text\n",
    "\n",
    "    article_text = article_text.lower()\n",
    "    return article_text \n",
    "\n",
    "def Load_Wiki_Train():\n",
    "    w = Load_Data('Curated/TRAIN_WIKI.csv')\n",
    "    wiki_train = []\n",
    "    for line in w:\n",
    "        try:\n",
    "            cik = line[0]\n",
    "            link = line[1].replace('\"','')\n",
    "            article = Load_Wiki(link)\n",
    "            wiki_train.append([cik,article])\n",
    "        except:\n",
    "            print(link)\n",
    "    with open('Curated/wiki_train','wb') as out:\n",
    "        pickle.dump(wiki_train, out)\n",
    "    print(len(wiki_train), \"Articles Loaded\")\n",
    "    \n",
    "def Split_Wiki():\n",
    "    with open('Curated/wiki_train','rb') as out:\n",
    "        wiki_train = pickle.load(out)\n",
    "\n",
    "    wiki_train_split = []\n",
    "    for line in wiki_train:\n",
    "        cik = line[0]\n",
    "        wik = line[1].split(\".\")\n",
    "        for l in wik:\n",
    "            wiki_train_split.append([line[0],l])\n",
    "\n",
    "    with open('Curated/wiki_train_split','wb') as out:\n",
    "        pickle.dump(wiki_train_split,out)\n",
    "        \n",
    "def Load_Wiki_Train_Test(f1,f2,f3):\n",
    "    trainx = Load_Data(f1)\n",
    "    test = Load_Data(f2)\n",
    "    with open(f3,'rb') as out:\n",
    "        wiki_train = pickle.load(out)\n",
    "    train = trainx + wiki_train\n",
    "    return train, test\n",
    "\n",
    "Load_Wiki_Train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and Transform Data into Keras-Readable Vector Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encode_Data(Load):\n",
    "    train, test = Load\n",
    "    \n",
    "    f = train + test\n",
    "    full_train = pd.DataFrame(train)\n",
    "    full_test = pd.DataFrame(test)\n",
    "    full = pd.DataFrame(f)\n",
    "\n",
    "    sentences_full = full[1].values\n",
    "    sentences_train = full_train[1].values\n",
    "    sentences_test = full_test[1].values\n",
    "    y_train = full_train[0].values\n",
    "    y_test = full_test[0].values\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_full)\n",
    "\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test = vectorizer.transform(sentences_test)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    train_encoded_Y = encoder.transform(y_train)\n",
    "    test_encoded_Y = encoder.transform(y_test)\n",
    "    tr_y = np_utils.to_categorical(train_encoded_Y)\n",
    "    te_y = np_utils.to_categorical(test_encoded_Y)\n",
    "    labels = [[w,x,y,z] for w,x,y,z in zip(test_encoded_Y,y_test,X_test,sentences_test)]\n",
    "    \n",
    "    return X_train, tr_y, X_test, te_y, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model - has only one input layer and one softmax layer for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Simple_Model(n,b,m,name):\n",
    "    print(\"Starting Model with \",str(n),\"epochs\")\n",
    "    start = time.time()\n",
    "    X_train, tr_y, X_test, te_y, labeled = m\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=X_train.get_shape()[1], activation='relu'))\n",
    "    model.add(Dense(tr_y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, tr_y, batch_size=b,epochs=n)\n",
    "    score = model.evaluate(X_test, te_y, batch_size=1)\n",
    "    model.save(name+'_Models//'+name+'_Simple_n'+str(n)+'_b'+str(b)+'_t'+str(start))\n",
    "    print(\"Score:\",score)\n",
    "    end = time.time()\n",
    "    print(\"Model with \",str(n),\"epochs took\",(end-start),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = [1000]\n",
    "for n in nlist:\n",
    "    Run_Simple_Model(n,1,Encode_Data(Load_Baseline('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv')),'Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Plus - Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(160,161,1):\n",
    "    Run_Simple_Model(n,16,Encode_Data(Load_Wiki_Train_Test('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv','Curated/wiki_train_split')),'EntityPlus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model - includes input layer, randomization layer, dense layer, and softmax layer for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Advanced_Model(n,b,m,name):\n",
    "    print(\"Starting Model with \",str(n),\"epochs\")\n",
    "    start = time.time()\n",
    "    X_train, tr_y, X_test, te_y, labeled = m\n",
    "    model = Sequential()\n",
    "    model.add(Dense(tr_y.shape[1], input_dim=X_train.get_shape()[1], activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(tr_y.shape[1], activation='relu'))\n",
    "    model.add(Dense(tr_y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, tr_y, batch_size=b,epochs=n)\n",
    "    score = model.evaluate(X_test, te_y, batch_size=1)\n",
    "    model.save(name+'_Models//'+name+'_Advanced_n'+str(n)+'_b'+str(b)+'_t'+str(start))\n",
    "    print(\"Score:\",score)\n",
    "    end = time.time()\n",
    "    print(\"Model with \",str(n),\"epochs took\",(end-start),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = [30]\n",
    "for n in nlist:\n",
    "    Run_Advanced_Model(n,1,Encode_Data(Load_Baseline('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv')),'Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Plus - Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(11,12,1):\n",
    "    Run_Advanced_Model(n,10,Encode_Data(Load_Wiki_Train_Test('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv','Curated/wiki_train_split')),'EntityPlus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Layers Model - includes input layer, four Randomization and dense layers, and softmax layer for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Many_Layer_Model(n,b,m,name):\n",
    "    print(\"Starting Model with \",str(n),\"epochs\")\n",
    "    start = time.time()\n",
    "    X_train, tr_y, X_test, te_y, labeled = m\n",
    "    model = Sequential()\n",
    "    model.add(Dense(tr_y.shape[1], input_dim=X_train.get_shape()[1], activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(tr_y.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(tr_y.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(tr_y.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(tr_y.shape[1], activation='relu'))\n",
    "    model.add(Dense(tr_y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, tr_y, batch_size=b,epochs=n)\n",
    "    score = model.evaluate(X_test, te_y, batch_size=1)\n",
    "    model.save(name+'_Models//'+name+'_Many_Layers_n'+str(n)+'_b'+str(b)+'_t'+str(start))\n",
    "    print(\"Score:\",score)\n",
    "    end = time.time()\n",
    "    print(\"Model with \",str(n),\"epochs took\",(end-start),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Many Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = [30]\n",
    "for n in nlist:\n",
    "    Run_Many_Layer_Model(n,1,Encode_Data(Load_Baseline('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv')),'Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Plus - Many Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(11,12,1):\n",
    "    Run_Many_Layer_Model(n,10,Encode_Data(Load_Wiki_Train_Test('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv','Curated/wiki_train_split')),'EntityPlus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Examine the Models and Analyze what the model got wrong. For the project the analysis was done in Excel (Confusion_Analysis.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExamineModel(f,m):\n",
    "    print(f)\n",
    "    model = keras.models.load_model(f)\n",
    "    X_train, tr_y, X_test, te_y, labeled = m\n",
    "    score = model.evaluate(X_test, te_y, batch_size=1)\n",
    "    print(score)\n",
    "    preds = model.predict(X_test,batch_size=1)\n",
    "    for row,l in zip(preds,labeled):\n",
    "        correct = l[0]\n",
    "        correct_Prob = row[correct]\n",
    "        cik = l[1]\n",
    "        name = l[3]\n",
    "        pred = row.argmax()\n",
    "        pred_Prob = row.max()\n",
    "        cat = 'Wrong'\n",
    "        if correct == pred:\n",
    "            cat = 'Correct'\n",
    "        else:\n",
    "            print([cik,correct,correct_Prob,pred,pred_Prob])\n",
    "            \n",
    "for file in glob.glob('Baseline_Models/*'):\n",
    "    try:\n",
    "        ExamineModel(file,Encode_Data(Load_Baseline('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv')))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for file in glob.glob('EntityPlus_Models/*'):\n",
    "    try:\n",
    "        ExamineModel(file,Encode_Data(Load_Wiki_Train_Test('Curated/TRAIN_NAME.csv','Curated/TEST_NAME.csv','Curated/wiki_train_split')))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
